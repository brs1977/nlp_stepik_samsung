{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"task3_cnn_postag_imbalanced.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3GVd8qEz7FOM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4b3babb8-76cb-43da-c1de-02e983660797","executionInfo":{"status":"ok","timestamp":1576218160810,"user_tz":-180,"elapsed":4144,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f0dubLh-vyZT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":457},"outputId":"67edd295-14e8-4e5d-c318-21aa63ef2dd5","executionInfo":{"status":"ok","timestamp":1576483888191,"user_tz":-180,"elapsed":7959,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git\n","import sys; sys.path.append('/content/stepik-dl-nlp')\n","\n","!pip install pyconll\n","!pip install spacy-udpipe "],"execution_count":1,"outputs":[{"output_type":"stream","text":["fatal: destination path 'stepik-dl-nlp' already exists and is not an empty directory.\n","Requirement already satisfied: pyconll in /usr/local/lib/python3.6/dist-packages (2.2.1)\n","Requirement already satisfied: requests>=2.21 in /usr/local/lib/python3.6/dist-packages (from pyconll) (2.21.0)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll) (1.24.3)\n","Requirement already satisfied: spacy-udpipe in /usr/local/lib/python3.6/dist-packages (0.0.4)\n","Requirement already satisfied: ufal.udpipe>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy-udpipe) (1.2.0.3)\n","Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy-udpipe) (2.1.9)\n","Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe) (0.4.2)\n","Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe) (0.2.4)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe) (0.9.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe) (2.21.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe) (1.17.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe) (2.0.3)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe) (2.0.1)\n","Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe) (7.0.8)\n","Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe) (0.2.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe) (1.0.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->spacy-udpipe) (2019.11.28)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->spacy-udpipe) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->spacy-udpipe) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->spacy-udpipe) (1.24.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy>=2.1.0->spacy-udpipe) (4.28.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DW7og61EwGT5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":404},"outputId":"4f6d1eed-810b-453f-ada6-f90cd9686bb0","executionInfo":{"status":"ok","timestamp":1576483892655,"user_tz":-180,"elapsed":4453,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["!wget -O /content/stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n","!wget -O /content/stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2019-12-16 08:11:29--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 81043533 (77M) [text/plain]\n","Saving to: ‘/content/stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu’\n","\n","/content/stepik-dl- 100%[===================>]  77.29M   273MB/s    in 0.3s    \n","\n","2019-12-16 08:11:30 (273 MB/s) - ‘/content/stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu’ saved [81043533/81043533]\n","\n","--2019-12-16 08:11:31--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10903424 (10M) [text/plain]\n","Saving to: ‘/content/stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu’\n","\n","/content/stepik-dl- 100%[===================>]  10.40M  --.-KB/s    in 0.08s   \n","\n","2019-12-16 08:11:32 (126 MB/s) - ‘/content/stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu’ saved [10903424/10903424]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9M6YHPcpwBXj","colab_type":"code","colab":{}},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.metrics import classification_report\n","\n","import numpy as np\n","\n","import pyconll\n","\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import TensorDataset\n","\n","import dlnlputils\n","from dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n","    character_tokenize, pos_corpus_to_tensor, POSTagger\n","from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n","import collections\n","import re\n","\n","init_random_seed()\n","\n","def build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n","    word_counts = collections.defaultdict(int)\n","    doc_n = 0\n","\n","    # посчитать количество документов, в которых употребляется каждое слово\n","    # а также общее количество документов\n","    for txt in tokenized_texts:\n","        doc_n += 1\n","        unique_text_tokens = set(txt)\n","        for token in unique_text_tokens:\n","            word_counts[token] += 1\n","\n","    # убрать слишком редкие и слишком частые слова\n","    word_counts = {word: cnt for word, cnt in word_counts.items()\n","                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n","\n","    # отсортировать слова по убыванию частоты\n","    sorted_word_counts = sorted(word_counts.items(),\n","                                reverse=True,\n","                                key=lambda pair: pair[1])\n","\n","    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n","    if pad_word is not None:\n","        #sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n","        sorted_word_counts = sorted_word_counts + [(pad_word, len(sorted_word_counts)+1)]\n","\n","    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n","    if len(word_counts) > max_size:\n","        sorted_word_counts = sorted_word_counts[:max_size]\n","\n","    # нумеруем слова\n","    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n","\n","    # нормируем частоты слов\n","    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n","\n","    return word2id, word2freq\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"maf9gKUDwMHq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7SlFPWBHwMK1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"2f8d5af1-2956-4d27-da23-1732cded3cb4","executionInfo":{"status":"ok","timestamp":1576483983014,"user_tz":-180,"elapsed":93102,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["full_train = pyconll.load_from_file('/content/stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu')\n","full_test = pyconll.load_from_file('/content/stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu')\n","\n","MAX_SENT_LEN = max(len(sent) for sent in full_train)\n","MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n","print('Наибольшая длина предложения', MAX_SENT_LEN)\n","print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)\n","\n","all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n","print('\\n'.join(all_train_texts[:10]))\n","\n","train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n","char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\n","print(\"Количество уникальных символов\", len(char_vocab))\n","print(list(char_vocab.items())[:10])\n","\n","UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n","label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n","\n","train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n","train_dataset = TensorDataset(train_inputs, train_labels)\n","\n","test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n","test_dataset = TensorDataset(test_inputs, test_labels)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Наибольшая длина предложения 205\n","Наибольшая длина токена 47\n","Анкета .\n","Начальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\n","В приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\n","Однако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\n","Приемная была обставлена просто , но по-деловому .\n","У двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\n","В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\n","Кабинет отличался скромностью , присущей Семену Еремеевичу .\n","В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\n","Справа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n","Количество уникальных символов 150\n","[(' ', 0), ('о', 1), ('е', 2), ('а', 3), ('т', 4), ('и', 5), ('н', 6), ('.', 7), ('с', 8), ('р', 9)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"szo3yK-WwoX8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TMgxbPVdgynI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"27c5fdf4-1368-494e-a679-9b2438c05c0b","executionInfo":{"status":"ok","timestamp":1576221719143,"user_tz":-180,"elapsed":1067,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["def get_class_weights(labels):\n","  count_labels = np.unique(labels, return_counts= True)[1]\n","  return count_labels/count_labels[1:].sum()\n","  #exclude 0 <PAD> weight\n","  # return count_labels[1:]/count_labels[1:].sum()\n","\n","def get_label_with_min_weight(labels, class_weights):\n","  labels = np.unique(labels)\n","  label_weight = {l:class_weights[l] for l in labels}\n","  return min(label_weight.items(), key=lambda item: item[1])\n","\n","class_weights = get_class_weights(train_labels)  \n","print(class_weights)\n","#get_label_with_min_weight(train_labels[349], class_weights)  \n","\n","def get_label_weight(labels, class_weights, n_weighted = 2):\n","  labels = np.unique(labels)\n","  label_weight = {l:class_weights[l] for l in labels}\n","  \n","  weights = sorted(label_weight.items(), key=lambda item: item[1])\n","  return sum(map(lambda x: x[1], weights[:n_weighted]))\n","\n","# array([9.13011861e-01, 8.55302407e-03, 8.19067301e-03, 4.40707234e-03,\n","#        7.51683593e-04, 3.04111076e-03, 2.19529183e-03, 7.79464508e-06,\n","#        2.14349742e-02, 1.37365630e-03, 2.66327033e-03, 3.84116112e-03,\n","#        3.23787558e-03, 1.57880536e-02, 1.62078652e-03, 8.39423316e-05,\n","#        9.76029468e-03, 3.74742552e-05])"],"execution_count":35,"outputs":[{"output_type":"stream","text":["[1.04958201e+01 9.83240262e-02 9.41585035e-02 5.06629109e-02\n"," 8.64121937e-03 3.49600622e-02 2.52366800e-02 8.96058377e-05\n"," 2.46412607e-01 1.57913057e-02 3.06164767e-02 4.41572973e-02\n"," 3.72220352e-02 1.81496624e-01 1.86322703e-02 9.64985945e-04\n"," 1.12202592e-01 4.30797297e-04]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pK7H85O4EUiQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"250698c0-dafe-4580-b9f7-a76aaaec1851","executionInfo":{"status":"ok","timestamp":1576221632920,"user_tz":-180,"elapsed":1050,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["\n","\n","# weights = get_label_weight(train_labels[349],class_weights)\n","# sum(weights, ke)\n","\n","class_weights = get_class_weights(train_labels)  \n","# weight_labels = [get_label_weight(label, class_weights)   for label in train_labels]\n","# weight_labels\n","class_weights\n"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([9.13011861e-01, 8.55302407e-03, 8.19067301e-03, 4.40707234e-03,\n","       7.51683593e-04, 3.04111076e-03, 2.19529183e-03, 7.79464508e-06,\n","       2.14349742e-02, 1.37365630e-03, 2.66327033e-03, 3.84116112e-03,\n","       3.23787558e-03, 1.57880536e-02, 1.62078652e-03, 8.39423316e-05,\n","       9.76029468e-03, 3.74742552e-05])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"xtYEzv-S7Rx-","colab_type":"code","colab":{}},"source":["import torch\n","import torch.utils.data\n","import torchvision\n","\n","def get_class_weights(labels):\n","  count_labels = np.unique(labels, return_counts= True)[1]\n","  return count_labels/count_labels.sum()\n","  #exclude 0 <PAD> weight\n","  # return count_labels[1:]/count_labels[1:].sum()\n","\n","def get_label_weight(labels, class_weights, n_weighted = 5):\n","  labels = np.unique(labels)\n","  label_weight = {l:class_weights[l] for l in labels}\n","  \n","  weights = sorted(label_weight.items(), key=lambda item: item[1] if item[0] != 0 else 0)\n","  return sum(map(lambda x: x[1], weights[:n_weighted]))\n","  \n","\n","\n","def get_label_with_min_weight(labels, class_weights):\n","  labels = np.unique(labels)\n","  label_weight = {l:class_weights[l] for l in labels}\n","  return min(label_weight.items(), key=lambda item: item[1])\n","\n","class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n","    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n","    Arguments:\n","        indices (list, optional): a list of indices\n","        num_samples (int, optional): number of samples to draw\n","    \"\"\"\n","\n","    def __init__(self, dataset):\n","                \n","        # if indices is not provided, \n","        # all elements in the dataset will be considered\n","        self.indices = np.arange(dataset.shape[0])\n","            \n","        # if num_samples is not provided, \n","        # draw `len(indices)` samples in each iteration\n","        self.num_samples = len(self.indices)\n","            \n","        class_weights = get_class_weights(dataset)\n","        # weight_labels = [get_label_with_min_weight(label, class_weights)   for label in dataset]\n","        weight_labels = [get_label_weight(label, class_weights)   for label in dataset]\n","\n","        # # weight for each sample\n","        self.weights = [weight_labels[idx] for idx in self.indices]\n","        self.weights = torch.DoubleTensor(self.weights)\n","                \n","    def __iter__(self):\n","        return (self.indices[i] for i in torch.multinomial(\n","            self.weights, self.num_samples, replacement=True))\n","\n","    def __len__(self):\n","        return self.num_samples"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hdh19nzo9l_u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"7dbc7ed2-a5a2-42a2-b79f-555e857ce488","executionInfo":{"status":"ok","timestamp":1576219182430,"user_tz":-180,"elapsed":1039,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["np.unique(train_labels, return_counts=True)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n","        17]),\n"," array([9136391,   85589,   81963,   44101,    7522,   30432,   21968,\n","             78,  214497,   13746,   26651,   38438,   32401,  157989,\n","          16219,     840,   97670,     375]))"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"Wrbif2LT-v5d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"438eb852-96ec-4df8-de53-b3ec00722562","executionInfo":{"status":"ok","timestamp":1576473733127,"user_tz":-180,"elapsed":2623,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["\n","# def get_class_weights(labels):\n","#   count_labels = np.unique(labels, return_counts= True)[1]\n","#   return count_labels/count_labels.sum()\n","#   #exclude 0 <PAD> weight\n","#   # return count_labels[1:]/count_labels[1:].sum()\n","\n","# def get_label_weight(labels, class_weights, n_weighted = 2):\n","#   labels = np.unique(labels)\n","#   label_weight = {l:class_weights[l] for l in labels}\n","  \n","#   weights = sorted(label_weight.items(), key=lambda item: item[1])\n","#   return sum(map(lambda x: x[1], weights[:n_weighted]))\n","\n","# num_samples = train_labels.shape[0]\n","# class_weights = get_class_weights(train_labels)  \n","# print(class_weights)\n","# weight_labels = [get_label_weight(label, class_weights, n_weighted = 5)   for label in train_labels]\n","# weights = [weight_labels[idx] for idx in np.arange(num_samples)]\n","\n","# #get_label_with_min_weight(train_labels[349], class_weights)  \n","\n","# #torch.utils.data.WeightedRandomSampler(weights, num_samples, replacement=True)\n","\n","sampler =  ImbalancedDatasetSampler(train_labels)\n","#sampler = torch.utils.data.WeightedRandomSampler(weights, num_samples)\n","ids = [x for x in sampler]\n","\n","np.unique(train_labels[ids], return_counts=True)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n","        17]),\n"," array([9140468,   85433,   81344,   44061,    7404,   30233,   21630,\n","             72,  213661,   13497,   26469,   38040,   32138,  157519,\n","          16095,     852,   97587,     367]))"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"JeJRP_fiHMOc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"6b9c2833-6370-47ac-bba6-9748a0359c4b","executionInfo":{"status":"ok","timestamp":1576221330709,"user_tz":-180,"elapsed":1089,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["array([9136391,   85589,   81963,   44101,    7522,   30432,   21968,\n","             78,  214497,   13746,   26651,   38438,   32401,  157989,\n","          16219,     840,   97670,     375]))"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([9.13011861e-01, 8.55302407e-03, 8.19067301e-03, 4.40707234e-03,\n","       7.51683593e-04, 3.04111076e-03, 2.19529183e-03, 7.79464508e-06,\n","       2.14349742e-02, 1.37365630e-03, 2.66327033e-03, 3.84116112e-03,\n","       3.23787558e-03, 1.57880536e-02, 1.62078652e-03, 8.39423316e-05,\n","       9.76029468e-03, 3.74742552e-05])"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"CNjO2_AgM14O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"dd60f788-0332-4de9-ee67-e57df5c96db1","executionInfo":{"status":"ok","timestamp":1576222829672,"user_tz":-180,"elapsed":1071,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["#np.log(1-4.30797297e-04)"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.00043089011681421034"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"7E3Cmiy8uF8x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"3abc49a2-a239-4880-f6d4-305c82820c50","executionInfo":{"status":"ok","timestamp":1576215632605,"user_tz":-180,"elapsed":627,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","#min_weight_labels = [get_label_with_min_weight(l, class_weights)[0] for l in train_labels]\n","\n","compute_class_weight('balanced',\n","      np.unique(min_weight_labels),\n","      min_weight_labels)\n","\n"],"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 4.1452106 ,  2.60091645,  2.17298789,  0.47139601,  0.72364208,\n","        0.39148916, 40.14309211,  0.41089226,  0.55887067,  1.38048643,\n","        1.35114039,  9.41628086,  0.3805507 ,  4.51981481,  8.76688218,\n","       13.3810307 ])"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"id":"4yEcEay-ttE_","colab_type":"code","colab":{}},"source":["\n","#min_weight_labels = [get_label_with_min_weight(label, class_weights)   for label in train_labels]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"miTzFcSLCvYK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"75b45cb8-75f9-42e9-c599-a4af55f14dc4","executionInfo":{"status":"ok","timestamp":1576220275965,"user_tz":-180,"elapsed":1127,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["weights[:2]\n","\n","sum(map(lambda x: x[1], weights[:2]))"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0038786353774956607"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"-2Imde8Ttlwb","colab_type":"code","colab":{}},"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","compute_class_weight('balanced',\n","      np.unique(train_labels[1]),\n","      train_labels[1])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BuCm7ry_wpn3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DiAXD3SoF6a0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":140},"outputId":"75704135-9914-4d0b-cde5-f9f0a0ad1165","executionInfo":{"status":"ok","timestamp":1576153919621,"user_tz":-180,"elapsed":1643,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["np.array(train_labels)"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 8, 13,  0, ...,  0,  0,  0],\n","       [ 8,  1,  8, ...,  0,  0,  0],\n","       [ 2,  8, 11, ...,  0,  0,  0],\n","       ...,\n","       [13,  2, 11, ...,  0,  0,  0],\n","       [13,  2, 11, ...,  0,  0,  0],\n","       [ 5, 11, 16, ...,  0,  0,  0]])"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"4vIeOOOpGSCB","colab_type":"code","colab":{}},"source":["list(train_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xoP93b4h7Plm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"outputId":"d9deee4b-ca3a-4f53-f980-b138381f308f","executionInfo":{"status":"error","timestamp":1576157473615,"user_tz":-180,"elapsed":1147,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","compute_class_weight('balanced',\n","      np.unique(train_labels[1]),\n","      train_labels[1])\n","\n","#classes = {k:k for k in np.arange(class_weigts.shape[0]) }\n","#compute_class_weight('balanced', classes, np.array(train_labels[0]))\n","#classes[3]\n","#compute_class_weight(train_labels[0])\n","\n","# indices = list(range(len(train_dataset))) \n","# num_samples = len(indices) \n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-80d4870cd694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m compute_class_weight('balanced',\n\u001b[0;32m----> 4\u001b[0;31m       \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m       train_labels[1])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"cell_type":"code","metadata":{"id":"hii4p5GD9tVp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":228},"outputId":"9e1863bd-738e-4a2b-c3a8-3c73a1fcbd02","executionInfo":{"status":"ok","timestamp":1576152736642,"user_tz":-180,"elapsed":1650,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["train_labels[0]\n","#np.arange(class_weigts.shape[0])"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 8, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0])"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"b92pr-xowpqv","colab_type":"code","colab":{}},"source":["class MyConv1d(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n","        super().__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.padding = padding\n","        self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n","                                   requires_grad=True)\n","        self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n","    \n","    def forward(self, x):\n","        \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n","\n","        batch_size, src_channels, sequence_len = x.shape        \n","        if self.padding > 0:\n","            pad = x.new_zeros(batch_size, src_channels, self.padding)\n","            x = torch.cat((pad, x, pad), dim=-1)\n","            sequence_len = x.shape[-1]\n","\n","        chunks = []\n","        chunk_size = sequence_len - self.kernel_size + 1\n","        for offset in range(self.kernel_size):\n","            chunks.append(x[:, :, offset:offset + chunk_size])\n","\n","        in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n","        in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n","        out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n","        out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n","        return out_features\n","\n","class StackedConv1d(nn.Module):\n","    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n","        super().__init__()\n","        layers = []\n","        for layer_n in range(layers_n):\n","            layers.append(nn.Sequential(\n","                #conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n","                #conv_layer(features_num-layer_n, features_num-layer_n, kernel_size, padding=kernel_size//2, dilation=layer_n+1),                \n","                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),                \n","                #nn.BatchNorm1d(features_num),\n","                nn.Dropout(dropout),                \n","                #nn.LeakyReLU()))\n","                nn.ReLU()))\n","        self.layers = nn.ModuleList(layers)\n","    \n","    def forward(self, x):\n","        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n","        for layer in self.layers:\n","            x = x + layer(x)\n","        return x\n","\n","class SentenceLevelPOSTagger(nn.Module):\n","    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n","        super().__init__()\n","        self.embedding_size = embedding_size\n","        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n","        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n","        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n","        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n","        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n","        self.labels_num = labels_num\n","    \n","    def forward(self, tokens):\n","        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n","        batch_size, max_sent_len, max_token_len = tokens.shape\n","        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n","        \n","        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n","        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n","        char_features = self.single_token_backbone(char_embeddings)\n","        \n","        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n","\n","        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n","        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n","        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n","\n","        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n","        return logits\n","\n","class SingleTokenPOSTagger(nn.Module):\n","    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n","        super().__init__()\n","        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n","        self.backbone = StackedConv1d(embedding_size, **kwargs)\n","        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n","        self.out = nn.Linear(embedding_size, labels_num)\n","        self.labels_num = labels_num\n","    \n","    def forward(self, tokens):\n","        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n","        batch_size, max_sent_len, max_token_len = tokens.shape\n","        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n","        \n","        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n","        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n","        \n","        features = self.backbone(char_embeddings)\n","        \n","        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n","        \n","        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n","        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n","        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n","        return logits\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LOgMLRKew8rL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"01dfc00b-7125-44e8-80b2-b6286231267c","executionInfo":{"status":"ok","timestamp":1576484043825,"user_tz":-180,"elapsed":700,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["sentence_level_model_my_conv = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=96,\n","                                                      single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.2, conv_layer=MyConv1d),\n","                                                      context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.2, conv_layer=MyConv1d))\n","                                                      # single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.2, conv_layer=nn.Conv1d),\n","                                                      # context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.2, conv_layer=nn.Conv1d))\n","\n","print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_my_conv.parameters()))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Количество параметров 182610\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fp4zrmW75nQa","colab_type":"code","colab":{}},"source":["from torch.utils.data import DataLoader\n","import copy\n","import datetime\n","import random\n","import traceback\n","\n","\n","def copy_data_to_device(data, device):\n","    if torch.is_tensor(data):\n","        return data.to(device)\n","    elif isinstance(data, (list, tuple)):\n","        return [copy_data_to_device(elem, device) for elem in data]\n","    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n","\n","\n","def train_eval_loop(model, train_dataset, val_dataset, criterion,\n","                    lr=1e-4, epoch_n=10, batch_size=32,\n","                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n","                    max_batches_per_epoch_train=10000,\n","                    max_batches_per_epoch_val=1000,\n","                    data_loader_ctor=DataLoader,\n","                    optimizer_ctor=None,\n","                    lr_scheduler_ctor=None,\n","                    shuffle_train=True,\n","                    dataloader_workers_n=0,\n","                    weights = None):\n","    \"\"\"\n","    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n","    :param model: torch.nn.Module - обучаемая модель\n","    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n","    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n","    :param criterion: функция потерь для настройки модели\n","    :param lr: скорость обучения\n","    :param epoch_n: максимальное количество эпох\n","    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n","    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n","    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n","        отсутствие улучшения модели, чтобы обучение продолжалось.\n","    :param l2_reg_alpha: коэффициент L2-регуляризации\n","    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n","    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n","    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n","        (по умолчанию torch.utils.data.DataLoader)\n","    :return: кортеж из двух элементов:\n","        - среднее значение функции потерь на валидации на лучшей эпохе\n","        - лучшая модель\n","    \"\"\"\n","    if device is None:\n","        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    device = torch.device(device)\n","    model.to(device)\n","    # if weights is not None:\n","    #   weights = torch.tensor(weights).to(device)\n","\n","    if optimizer_ctor is None:\n","        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n","    else:\n","        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n","\n","    if lr_scheduler_ctor is not None:\n","        lr_scheduler = lr_scheduler_ctor(optimizer)\n","    else:\n","        lr_scheduler = None\n","\n","    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n","                                        num_workers=dataloader_workers_n)\n","    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n","                                      num_workers=dataloader_workers_n)\n","\n","    best_val_loss = float('inf')\n","    best_epoch_i = 0\n","    best_model = copy.deepcopy(model)\n","\n","    for epoch_i in range(epoch_n):\n","        try:\n","            epoch_start = datetime.datetime.now()\n","            print('Эпоха {}'.format(epoch_i))\n","\n","            model.train()\n","            mean_train_loss = 0\n","            train_batches_n = 0\n","            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n","                if batch_i > max_batches_per_epoch_train:\n","                    break\n","\n","                batch_x = copy_data_to_device(batch_x, device)\n","                batch_y = copy_data_to_device(batch_y, device)\n","\n","                pred = model(batch_x)\n","                loss = criterion(pred, batch_y)\n","                # if not weights is None:\n","                #   loss = (loss * weights).mean()\n","\n","                model.zero_grad()\n","                loss.backward()\n","\n","                optimizer.step()\n","\n","                mean_train_loss += float(loss)\n","                train_batches_n += 1\n","\n","            mean_train_loss /= train_batches_n\n","            print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n,\n","                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n","            print('Среднее значение функции потерь на обучении', mean_train_loss)\n","\n","\n","\n","            model.eval()\n","            mean_val_loss = 0\n","            val_batches_n = 0\n","\n","            with torch.no_grad():\n","                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n","                    if batch_i > max_batches_per_epoch_val:\n","                        break\n","\n","                    batch_x = copy_data_to_device(batch_x, device)\n","                    batch_y = copy_data_to_device(batch_y, device)\n","\n","                    pred = model(batch_x)\n","                    loss = criterion(pred, batch_y)\n","\n","                    mean_val_loss += float(loss)\n","                    val_batches_n += 1\n","\n","            mean_val_loss /= val_batches_n\n","            print('Среднее значение функции потерь на валидации', mean_val_loss)\n","\n","            if mean_val_loss < best_val_loss:\n","                best_epoch_i = epoch_i\n","                best_val_loss = mean_val_loss\n","                best_model = copy.deepcopy(model)\n","                print('Новая лучшая модель!')\n","            elif epoch_i - best_epoch_i > early_stopping_patience:\n","                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n","                    early_stopping_patience))\n","                break\n","\n","            if lr_scheduler is not None:\n","                lr_scheduler.step(mean_val_loss)\n","\n","            print()\n","        except KeyboardInterrupt:\n","            print('Досрочно остановлено пользователем')\n","            break\n","        except Exception as ex:\n","            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n","            break\n","\n","    return best_val_loss, best_model\n","\n","class WeightedMultilabelCrossEntropy(nn.Module):  \n","    def __init__(self, weights: torch.Tensor):  \n","        super(WeightedMultilabelCrossEntropy, self).__init__()  \n","        self.cerition = F.cross_entropy #nn.BCEWithLogitsLoss(reduction='none')  \n","        self.weights = weights  \n","  \n","    def forward(self, outputs, targets):  \n","        loss = self.cerition(outputs, targets)  \n","        return (loss * self.weights).mean()    \n","\n","def get_class_weights(labels):\n","  count_labels = np.unique(labels, return_counts= True)[1]\n","  return count_labels/count_labels.sum()\n","  #exclude 0 <PAD> weight\n","  # return count_labels[1:]/count_labels[1:].sum()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"99Vc4EEhMdqZ","colab_type":"code","colab":{}},"source":["target = torch.tensor([[0,1,0,1,0,0]], dtype=torch.float32)\n","output = torch.randn(1, 6, requires_grad=True)\n","weights = torch.tensor([0.16, 0.16, 0.25, 0.25, 0.083, 0.083])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"33w9YPpXxBH8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"18d8ed84-75ac-42ae-f6e9-a67f1ddc2062","executionInfo":{"status":"ok","timestamp":1576491599475,"user_tz":-180,"elapsed":1209543,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["\n","\n","from sklearn.utils import class_weight\n","list_classes = np.arange(len(label2id))\n","y = train_labels.view(-1) #train_labels[list_classes].values\n","weights = class_weight.compute_sample_weight('balanced', y) #0.92\n","weights = torch.tensor(weights).to('cuda')\n","criterion = WeightedMultilabelCrossEntropy(weights) #0.93\n","#last <PAD> 0.93\n","\n","# weights = get_class_weights(train_labels) #0.91\n","# weights = torch.tensor(weights).to('cuda')\n","# criterion = WeightedMultilabelCrossEntropy(weights) #0.93\n","#last <PAD> 0.92\n","\n","(best_val_loss,\n"," best_sentence_level_model_my_conv) = train_eval_loop(sentence_level_model_my_conv,\n","                                                      train_dataset,\n","                                                      test_dataset,\n","                                                      F.cross_entropy, #criterion, #F.cross_entropy,\n","                                                      #weights=weights,\n","                                                      lr=5e-3,\n","                                                      epoch_n=10,\n","                                                      batch_size=64,\n","                                                      device='cuda',\n","                                                      early_stopping_patience=5,\n","                                                      max_batches_per_epoch_train=500,\n","                                                      max_batches_per_epoch_val=100,\n","                                                      lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n","                                                                                                                                 factor=0.5,\n","                                                                                                                                 verbose=True))\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Эпоха 0\n","Эпоха: 501 итераций, 113.93 сек\n","Среднее значение функции потерь на обучении 0.009429648394014188\n","Среднее значение функции потерь на валидации 0.008016423052545674\n","Новая лучшая модель!\n","\n","Эпоха 1\n","Эпоха: 501 итераций, 113.84 сек\n","Среднее значение функции потерь на обучении 0.00934646191900808\n","Среднее значение функции потерь на валидации 0.008078449617813128\n","\n","Эпоха 2\n","Эпоха: 501 итераций, 113.77 сек\n","Среднее значение функции потерь на обучении 0.00919034681143518\n","Среднее значение функции потерь на валидации 0.007809439683240829\n","Новая лучшая модель!\n","\n","Эпоха 3\n","Эпоха: 501 итераций, 113.75 сек\n","Среднее значение функции потерь на обучении 0.009180242095343367\n","Среднее значение функции потерь на валидации 0.008346723941256209\n","\n","Эпоха 4\n","Эпоха: 501 итераций, 113.73 сек\n","Среднее значение функции потерь на обучении 0.008950872356982942\n","Среднее значение функции потерь на валидации 0.007819625173261997\n","\n","Эпоха 5\n","Эпоха: 501 итераций, 113.70 сек\n","Среднее значение функции потерь на обучении 0.00909251994927397\n","Среднее значение функции потерь на валидации 0.008086948224463233\n","Epoch     5: reducing learning rate of group 0 to 2.5000e-03.\n","\n","Эпоха 6\n","Эпоха: 501 итераций, 113.73 сек\n","Среднее значение функции потерь на обучении 0.007644802432306513\n","Среднее значение функции потерь на валидации 0.006917144093514964\n","Новая лучшая модель!\n","\n","Эпоха 7\n","Эпоха: 501 итераций, 113.78 сек\n","Среднее значение функции потерь на обучении 0.0073019864573004006\n","Среднее значение функции потерь на валидации 0.006915299297946662\n","Новая лучшая модель!\n","\n","Эпоха 8\n","Эпоха: 501 итераций, 113.76 сек\n","Среднее значение функции потерь на обучении 0.0072425180147336924\n","Среднее значение функции потерь на валидации 0.0068271098782926205\n","Новая лучшая модель!\n","\n","Эпоха 9\n","Эпоха: 501 итераций, 113.80 сек\n","Среднее значение функции потерь на обучении 0.007142884282213068\n","Среднее значение функции потерь на валидации 0.006824374406386425\n","Новая лучшая модель!\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t4IT9e7mxOu7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":985},"outputId":"f1534ae9-44f9-49b1-d2f7-ad0878f79375","executionInfo":{"status":"ok","timestamp":1576491683981,"user_tz":-180,"elapsed":83843,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["train_pred = predict_with_model(best_sentence_level_model_my_conv, train_dataset)\n","train_loss = F.cross_entropy(torch.tensor(train_pred),\n","                             torch.tensor(train_labels))\n","print('Среднее значение функции потерь на обучении', float(train_loss))\n","print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n","print()\n","\n","test_pred = predict_with_model(best_sentence_level_model_my_conv, test_dataset)\n","test_loss = F.cross_entropy(torch.tensor(test_pred),\n","                            torch.tensor(test_labels))\n","print('Среднее значение функции потерь на валидации', float(test_loss))\n","print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["1526it [00:53, 28.50it/s]                               \n"],"name":"stderr"},{"output_type":"stream","text":["Среднее значение функции потерь на обучении 0.00495589803904295\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|▏         | 3/205.75 [00:00<00:07, 25.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","     <NOTAG>       1.00      1.00      1.00   9136391\n","         ADJ       0.97      0.97      0.97     85589\n","         ADP       1.00      1.00      1.00     81963\n","         ADV       0.96      0.95      0.96     44101\n","         AUX       0.87      0.97      0.92      7522\n","       CCONJ       0.95      0.98      0.97     30432\n","         DET       0.96      0.91      0.93     21968\n","        INTJ       0.92      0.62      0.74        78\n","        NOUN       0.99      0.99      0.99    214497\n","         NUM       0.96      0.98      0.97     13746\n","        PART       0.96      0.93      0.95     26651\n","        PRON       0.94      0.96      0.95     38438\n","       PROPN       0.98      0.98      0.98     32401\n","       PUNCT       1.00      1.00      1.00    157989\n","       SCONJ       0.91      0.95      0.93     16219\n","         SYM       1.00      1.00      1.00       840\n","        VERB       0.98      0.97      0.98     97670\n","           X       0.98      0.73      0.84       375\n","\n","    accuracy                           1.00  10006870\n","   macro avg       0.96      0.94      0.95  10006870\n","weighted avg       1.00      1.00      1.00  10006870\n","\n","\n"],"name":"stdout"},{"output_type":"stream","text":["206it [00:07, 28.32it/s]                            \n"],"name":"stderr"},{"output_type":"stream","text":["Среднее значение функции потерь на валидации 0.006752926856279373\n","              precision    recall  f1-score   support\n","\n","     <NOTAG>       1.00      1.00      1.00   1231232\n","         ADJ       0.96      0.96      0.96     11222\n","         ADP       1.00      1.00      1.00     10585\n","         ADV       0.96      0.94      0.95      6165\n","         AUX       0.86      0.96      0.91      1106\n","       CCONJ       0.95      0.98      0.97      4410\n","         DET       0.94      0.89      0.91      3085\n","        INTJ       0.67      0.36      0.47        11\n","        NOUN       0.98      0.98      0.98     27974\n","         NUM       0.96      0.98      0.97      1829\n","        PART       0.96      0.93      0.94      3877\n","        PRON       0.93      0.95      0.94      5598\n","       PROPN       0.96      0.95      0.96      4438\n","       PUNCT       1.00      1.00      1.00     22694\n","       SCONJ       0.90      0.94      0.92      2258\n","         SYM       1.00      1.00      1.00        53\n","        VERB       0.98      0.97      0.97     13078\n","           X       0.97      0.81      0.88       105\n","\n","    accuracy                           1.00   1349720\n","   macro avg       0.94      0.92      0.93   1349720\n","weighted avg       1.00      1.00      1.00   1349720\n","\n"],"name":"stdout"}]}]}