{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer_bertabs.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPygteCoxeEXlSmZVKAHctZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VIeQGZE1TMqz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"630f0175-1788-4890-def4-820d5d4f9fb8","executionInfo":{"status":"ok","timestamp":1580275432766,"user_tz":-180,"elapsed":18327,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["from google.colab import drive, files\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G80jrSIxxwN0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":811},"outputId":"d69a9755-0003-4d85-f4cd-9a2fa0eb418e","executionInfo":{"status":"ok","timestamp":1580275443659,"user_tz":-180,"elapsed":13408,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["!pip install transformers\n","!pip install py-rouge\n","\n","!git clone https://github.com/brs1977/transformers_summarization.git trans_bertsum"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n","\r\u001b[K     |▊                               | 10kB 26.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 174kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 194kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 225kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 245kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 296kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 317kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 337kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 348kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 368kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 389kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 399kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 409kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 440kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 3.5MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 55.0MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n","\u001b[K     |████████████████████████████████| 870kB 46.4MB/s \n","\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=b5755641aee56076f7db1acbd7ad43e33bcff1997904043aa4f298a84f8351b8\n","  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n","Collecting py-rouge\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/1d/0bdbaf559fb7afe32308ebc84a2028600988212d7eb7fb9f69c4e829e4a0/py_rouge-1.1-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 2.3MB/s \n","\u001b[?25hInstalling collected packages: py-rouge\n","Successfully installed py-rouge-1.1\n","Cloning into 'trans_bertsum'...\n","remote: Enumerating objects: 18, done.\u001b[K\n","remote: Counting objects: 100% (18/18), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 18 (delta 4), reused 17 (delta 3), pack-reused 0\u001b[K\n","Unpacking objects: 100% (18/18), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x4bnJC-ez8QY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4736a11a-a911-4f2a-beb1-c86b4cb1a105","executionInfo":{"status":"ok","timestamp":1580275447342,"user_tz":-180,"elapsed":10354,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["#kaggle key\n","!mkdir ~/.kaggle\n","!cp /content/drive/My\\ Drive/kaggle.json ~/.kaggle\n","!ls ~/.kaggle"],"execution_count":3,"outputs":[{"output_type":"stream","text":["kaggle.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GvkrC4p1ltdL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":246},"outputId":"ea68b557-f645-49a7-b225-593af7937422","executionInfo":{"status":"ok","timestamp":1580275451746,"user_tz":-180,"elapsed":12183,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["!kaggle competitions download -c title-generation"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n","Downloading vocs.pkl.zip to /content\n","  0% 0.00/6.39M [00:00<?, ?B/s]\n","100% 6.39M/6.39M [00:00<00:00, 58.8MB/s]\n","Downloading test.csv to /content\n","  0% 0.00/905k [00:00<?, ?B/s]\n","100% 905k/905k [00:00<00:00, 129MB/s]\n","Downloading train.csv.zip to /content\n"," 80% 35.0M/43.7M [00:00<00:00, 67.2MB/s]\n","100% 43.7M/43.7M [00:00<00:00, 97.8MB/s]\n","Downloading sample_submission.csv.zip to /content\n","  0% 0.00/778k [00:00<?, ?B/s]\n","100% 778k/778k [00:00<00:00, 107MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gl0QiBOClxoe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"9c02e9eb-6d09-4979-ed6f-b5025759cc9a","executionInfo":{"status":"ok","timestamp":1580275458129,"user_tz":-180,"elapsed":15151,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["#%cd /content\n","!mkdir data\n","!unzip sample_submission.csv.zip -d data\n","!unzip vocs.pkl.zip -d data\n","!unzip train.csv.zip -d data\n","!mv test.csv data"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Archive:  sample_submission.csv.zip\n","  inflating: data/sample_submission.csv  \n","Archive:  vocs.pkl.zip\n","  inflating: data/vocs.pkl           \n","Archive:  train.csv.zip\n","  inflating: data/train.csv          \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f0i2Vkacx7z7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":81},"outputId":"2dcde5de-0616-479f-bb31-d5fea909c085","executionInfo":{"status":"ok","timestamp":1580275463238,"user_tz":-180,"elapsed":17827,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["%cd /content/trans_bertsum\n","import numpy as np\n","\n","import random\n","import argparse\n","import logging\n","import os\n","import sys\n","from collections import namedtuple\n","\n","import data_loader\n","from data_loader import load_dataset\n","\n","import torch\n","from torch.utils.data import DataLoader, SequentialSampler\n","from tqdm import tqdm\n","\n","from modeling_bertabs import BertAbs, build_predictor\n","from transformers import BertTokenizer\n","from utils_dataset import (\n","    SummarizationDataset,\n","    build_mask,\n","    compute_token_type_ids,\n","    encode_for_summarization,\n","    fit_to_block_size,\n",")\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from tqdm import tqdm, trange\n","from transformers import (\n","    AdamW,\n","    get_linear_schedule_with_warmup)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/trans_bertsum\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"1M0DcrE6nLIH","colab_type":"code","colab":{}},"source":["# os.path.isdir(path)\n","#args.batch_size = 32"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqLBzcRjWQW2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTLSoSohx724","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b0aa0306-c19c-4a06-94f3-958d6bb3442f","executionInfo":{"status":"ok","timestamp":1580275502938,"user_tz":-180,"elapsed":44798,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                    datefmt='%m/%d/%Y %H:%M:%S',\n","                    level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","def set_seed(args):\n","  random.seed(args.seed)\n","  np.random.seed(args.seed)\n","  torch.manual_seed(args.seed)\n","\n","class ARGS(object):  \n","  model_name_or_path = ''         #\"Path to pre-trained model or shortcut name\"\n","  output_dir = ''                 # \"The output directory where the model checkpoints and predictions will be written.\"\n","  config_name = ''                #\"Pretrained config name or path if not the same as model_name\"\n","  seed = 666\n","  bert_model = \"bert-base-uncased\"\n","\n","  train_batch_size = 32\n","  num_train_epochs = 3            #\"Total number of training epochs to perform.\"\n","  max_steps = -1                  #\"If > 0: set total number of training steps to perform. Override num_train_epochs.\"\n","  logging_steps = 100             #\"Log every X updates steps.\"\n","  save_steps = 3600               #\"Save checkpoint every X updates steps.\"\n","\n","  gradient_accumulation_steps = 8 #\"Number of updates steps to accumulate before performing a backward/update pass.\"\n","  learning_rate = 5e-5            #\"The initial learning rate for Adam.\"\n","  adam_epsilon = 1e-8             #\"Epsilon for Adam optimizer.\"\n","  warmup_steps = 0.0              #\"Linear warmup over warmup_steps.\"   \n","  max_grad_norm = 1.0             #\"Max gradient norm.\"\n","  weight_decay = 0.0              #\"Weight decay if we apply some.\"\n","\n","  # n_best_size = 5                 #\"The total number of n-best predictions to generate in the nbest_predictions.json output file.\"\n","\n","  fp16 = False #\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\"\n","  fp16_opt_level = 'O1' #\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\" \"See details at https://nvidia.github.io/apex/amp.html\"\n","\n","  bert_data_path = 'data/arxiv'\n","  max_pos = 512\n","  max_tgt_len = 140\n","  use_interval = True\n","  task = 'abs'\n","\n","\n","args = ARGS()\n","\n","tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=True)\n","#tokenizer = BertTokenizer.from_pretrained(os.path.join(args.bert_model, 'vocab.txt'))\n","\n","train_dataset = torch.load('/content/drive/My Drive/nlp/nlp_model/abs_bert/data130X30/train.pt')\n","model = BertAbs.from_pretrained(\"bertabs-finetuned-cnndm\")\n","# model = BertAbs.from_pretrained(args.bert_model)\n","\n","args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(args.device)\n","\n","# Set seed\n","set_seed(args)\n","\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["01/29/2020 05:24:23 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpqywxe6ve\n","01/29/2020 05:24:23 - INFO - transformers.file_utils -   copying /tmp/tmpqywxe6ve to cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","01/29/2020 05:24:23 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","01/29/2020 05:24:23 - INFO - transformers.file_utils -   removing temp file /tmp/tmpqywxe6ve\n","01/29/2020 05:24:23 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","01/29/2020 05:24:26 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization-config.json not found in cache or force_download set to True, downloading to /tmp/tmp54v90ndg\n","01/29/2020 05:24:26 - INFO - transformers.file_utils -   copying /tmp/tmp54v90ndg to cache at /root/.cache/torch/transformers/7ebb4ac81007d10b400cb6c2968d4c8f1275a3e0cc3bab7f20f81913198b542c.df616398f4c84def6fca83d755543b01cb445db4ddd218d3efeded8ded68332f\n","01/29/2020 05:24:26 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/7ebb4ac81007d10b400cb6c2968d4c8f1275a3e0cc3bab7f20f81913198b542c.df616398f4c84def6fca83d755543b01cb445db4ddd218d3efeded8ded68332f\n","01/29/2020 05:24:26 - INFO - transformers.file_utils -   removing temp file /tmp/tmp54v90ndg\n","01/29/2020 05:24:26 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization-config.json from cache at /root/.cache/torch/transformers/7ebb4ac81007d10b400cb6c2968d4c8f1275a3e0cc3bab7f20f81913198b542c.df616398f4c84def6fca83d755543b01cb445db4ddd218d3efeded8ded68332f\n","01/29/2020 05:24:26 - INFO - transformers.configuration_utils -   Model config {\n","  \"dec_dropout\": 0.2,\n","  \"dec_ff_size\": 2048,\n","  \"dec_heads\": 8,\n","  \"dec_hidden_size\": 768,\n","  \"dec_layers\": 6,\n","  \"enc_dropout\": 0.2,\n","  \"enc_ff_size\": 512,\n","  \"enc_heads\": 8,\n","  \"enc_hidden_size\": 512,\n","  \"enc_layers\": 6,\n","  \"finetuning_task\": null,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"is_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"max_pos\": 512,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","01/29/2020 05:24:26 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpfnez5obi\n","01/29/2020 05:24:44 - INFO - transformers.file_utils -   copying /tmp/tmpfnez5obi to cache at /root/.cache/torch/transformers/6f1af625ee57a9fbf093ef0863fb774fbdae89fa99fea7a213c08ad26f0724c0.ef06f4d767c6fad3c61125520f9dbb0f219834539c0369980ee5ecb9d1ef5542\n","01/29/2020 05:24:47 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/6f1af625ee57a9fbf093ef0863fb774fbdae89fa99fea7a213c08ad26f0724c0.ef06f4d767c6fad3c61125520f9dbb0f219834539c0369980ee5ecb9d1ef5542\n","01/29/2020 05:24:47 - INFO - transformers.file_utils -   removing temp file /tmp/tmpfnez5obi\n","01/29/2020 05:24:47 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization-pytorch_model.bin from cache at /root/.cache/torch/transformers/6f1af625ee57a9fbf093ef0863fb774fbdae89fa99fea7a213c08ad26f0724c0.ef06f4d767c6fad3c61125520f9dbb0f219834539c0369980ee5ecb9d1ef5542\n","01/29/2020 05:24:47 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp5bnir8i5\n","01/29/2020 05:24:47 - INFO - transformers.file_utils -   copying /tmp/tmp5bnir8i5 to cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n","01/29/2020 05:24:47 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n","01/29/2020 05:24:47 - INFO - transformers.file_utils -   removing temp file /tmp/tmp5bnir8i5\n","01/29/2020 05:24:47 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n","01/29/2020 05:24:47 - INFO - transformers.configuration_utils -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wWLQEBmICLC0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"e0b3c3d3-65d5-4feb-e65c-6ae7021f311e","executionInfo":{"status":"ok","timestamp":1580280603463,"user_tz":-180,"elapsed":2122,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["!git pull origin master"],"execution_count":36,"outputs":[{"output_type":"stream","text":["From https://github.com/brs1977/transformers_summarization\n"," * branch            master     -> FETCH_HEAD\n","Already up to date.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ksh4z6qWy8qO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7e1e5dbd-7319-4483-8303-96f150225a09","executionInfo":{"status":"ok","timestamp":1580282404520,"user_tz":-180,"elapsed":570,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["# len(train_dataset[0])\n","# train_sampler = RandomSampler(train_dataset)\n","# train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, drop_last=True)\n","# for batch in train_dataloader:\n","#   break\n","# batch  \n","\n","\n","# data_loader.logger = logger\n","args.max_tgt_len = 96\n","train_dataloader = data_loader.Dataloader(args, load_dataset(args, 'train', shuffle=True), args.train_batch_size, args.device,\n","                                      shuffle=True, is_test=False)\n","\n"],"execution_count":54,"outputs":[{"output_type":"stream","text":["01/29/2020 07:20:04 - INFO - __main__ -   Loading train dataset from data/arxiv.train.6.bert.pt, number of examples: 1419\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"CzaO0Af5V5qR","colab_type":"code","colab":{}},"source":["Batch = namedtuple(\"Batch\", [\"document_names\", \"batch_size\", \"src\", \"segs\", \"mask_src\", \"tgt_str\"])\n","\n","def build_data_iterator(tokenizer):\n","    dataset = load_and_cache_examples(tokenizer)\n","    sampler = SequentialSampler(dataset)\n","\n","    def collate_fn(data):\n","        return collate(data, tokenizer, block_size=512, device=args.device)\n","\n","    iterator = DataLoader(dataset, sampler=sampler, batch_size=args.train_batch_size, collate_fn=collate_fn,)\n","\n","    return iterator\n","\n","\n","def load_and_cache_examples(tokenizer):\n","    dataset = SummarizationDataset('/content/data/')\n","    return dataset\n","\n","\n","def collate(data, tokenizer, block_size, device):\n","    \"\"\" Collate formats the data passed to the data loader.\n","\n","    In particular we tokenize the data batch after batch to avoid keeping them\n","    all in memory. We output the data as a namedtuple to fit the original BertAbs's\n","    API.\n","    \"\"\"\n","    data = [x for x in data if not len(x[1]) == 0]  # remove empty_files\n","    names = [name for name, _, _ in data]\n","    summaries = [\" \".join(summary_list) for _, _, summary_list in data]\n","\n","    encoded_text = [encode_for_summarization(abstract, title, tokenizer) for _, abstract, title in data]\n","    encoded_stories = torch.tensor(\n","        [fit_to_block_size(abstract, block_size, tokenizer.pad_token_id) for abstract, _ in encoded_text]\n","    )\n","    encoder_token_type_ids = compute_token_type_ids(encoded_stories, tokenizer.cls_token_id)\n","    encoder_mask = build_mask(encoded_stories, tokenizer.pad_token_id)\n","\n","    batch = Batch(\n","        document_names=names,\n","        batch_size=len(encoded_stories),\n","        src=encoded_stories, #.to(device),\n","        segs=encoder_token_type_ids, #.to(device),\n","        mask_src=encoder_mask, #.to(device),\n","        tgt_str=summaries,\n","    )\n","\n","    return batch\n","\n","data_iterator = build_data_iterator(tokenizer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gk2SD5zIRduD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5af4c292-b7c2-4f96-961a-69d78d388cc6","executionInfo":{"status":"ok","timestamp":1580034676949,"user_tz":-180,"elapsed":2165,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["\n","# !git pull origin master\n","\n","import torch\n","for batch in data_iterator:\n","  print(len(batch))\n","  break"],"execution_count":30,"outputs":[{"output_type":"stream","text":["6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nRXldfXfWej-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8b4c207f-be8d-4256-e7a2-16cc58b702bd","executionInfo":{"status":"ok","timestamp":1580278662261,"user_tz":-180,"elapsed":585,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["# !mkdir data\n","# !unzip /content/drive/My\\ Drive/nlp/pre_sum_data.zip -d /content/trans_bertsum/data/\n","\n","# dict_keys(['src', 'tgt', 'src_sent_labels', 'segs', 'clss', 'src_txt', 'tgt_txt'])\n","#train_data  = torch.load('/content/trans_bertsum/data/arxiv.train.0.bert.pt')\n","\n","train_data[0]['src_sent_labels']\n","# encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask, decoder_attention_mask\n","\n","#src, tgt, segs, clss, mask_src, mask_tgt, mask_cls\n","\n","# train_data  = torch.load('/content/drive/My Drive/nlp/nlp_model/abs_bert/data130X30/train.pt')\n","# train_sampler = RandomSampler(train_data)\n","# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size, drop_last=True)\n"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 0, 0, 0, 0, 0, 0, 0]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"eKYFI2rvx76A","colab_type":"code","colab":{}},"source":["def train(args, train_dataset, model, tokenizer):\n","    \"\"\" Train the model \"\"\"\n","    # train_sampler = RandomSampler(train_dataset)\n","    # train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n","\n","    if args.max_steps > 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n","    else:\n","        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n","    )\n","\n","    # Check if saved optimizer or scheduler states exist\n","    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n","        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n","    ):\n","        # Load in optimizer and scheduler states\n","        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n","        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n","\n","    if args.fp16:\n","        try:\n","            from apex import amp\n","        except ImportError:\n","            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n","\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_dataset))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 1\n","    epochs_trained = 0\n","    steps_trained_in_current_epoch = 0\n","    # Check if continuing training from a checkpoint\n","    if os.path.exists(args.model_name_or_path):\n","        try:\n","            # set global_step to gobal_step of last saved checkpoint from model path\n","            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n","            global_step = int(checkpoint_suffix)\n","            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n","            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n","\n","            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n","            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n","            logger.info(\"  Continuing training from global step %d\", global_step)\n","            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n","        except ValueError:\n","            logger.info(\"  Starting fine-tuning.\")\n","\n","    tr_loss, logging_loss = 0.0, 0.0\n","    model.zero_grad()\n","    train_iterator = trange(\n","        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=False\n","    )\n","    # Added here for reproductibility\n","    set_seed(args)\n","\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n","        for step, batch in enumerate(epoch_iterator):\n","\n","            # Skip past any already trained steps if resuming training\n","            if steps_trained_in_current_epoch > 0:\n","                steps_trained_in_current_epoch -= 1\n","                continue\n","\n","            model.train()\n","            batch = tuple(t.to(args.device) for t in batch)\n","            inputs = {\n","                \"encoder_input_ids\": batch[0],\n","                \"decoder_input_ids\": batch[1],\n","                \"token_type_ids\": batch[2],\n","                \"encoder_attention_mask\": batch[3],\n","                'decoder_attention_mask': batch[3]\n","            }\n","\n","    # def forward(self, src, src_mask, tgt, tgt_mask):\n","    #     # src/tgt: [batch_size, seq_len]\n","\n","    #     # shift right\n","    #     tgt = tgt[:, :-1]\n","    #     tgt_mask = tgt_mask[:, :-1]\n","    #     # bert input: BertModel.forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True)\n","    #     # token_type_ids is not important since we only have one sentence so we can use default all zeros\n","    #     bert_encoded = self.bert_encoder(src, attention_mask=src_mask, output_all_encoded_layers=False)[0]  # [batch_size, seq_len, hidden_size]\n","    #     # transformer input: BertDecoder.forward(self, tgt_seq_embedded, tgt_pos, src_seq, enc_output, return_attns=False)\n","    #     logits = self.decoder(tgt, src, bert_encoded)  # [batch_size, seq_len, vocab_size]\n","\n","\n","            outputs = model(**inputs)\n","            # model outputs are always tuple in transformers (see doc)\n","            loss = outputs[0]\n","\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                if args.fp16:\n","                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n","                else:\n","                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                # Log metrics\n","                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n","                    \n","                    # Only evaluate when single GPU otherwise metrics may not average well\n","                    # if args.evaluate_during_training:\n","                    #     results = evaluate(args, model, tokenizer)\n","                    #     for key, value in results.items():\n","                    #         tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n","                    \n","                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n","                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n","                    logging_loss = tr_loss\n","\n","                # Save model checkpoint\n","                if args.save_steps > 0 and global_step % args.save_steps == 0:\n","                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    # Take care of distributed/parallel training\n","                    model_to_save = model.module if hasattr(model, \"module\") else model\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n","\n","                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n","\n","            if args.max_steps > 0 and global_step > args.max_steps:\n","                epoch_iterator.close()\n","                break\n","        if args.max_steps > 0 and global_step > args.max_steps:\n","            train_iterator.close()\n","            break\n","\n","    tb_writer.close()\n","\n","    return global_step, tr_loss / global_step\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iishe6DXXJDd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":889},"outputId":"2401cc7d-8ac1-435b-9f01-396fe8e7a359","executionInfo":{"status":"error","timestamp":1580035369886,"user_tz":-180,"elapsed":566,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["train(args, data_iterator, model, tokenizer)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["01/26/2020 10:42:49 - INFO - __main__ -   ***** Running training *****\n","01/26/2020 10:42:49 - INFO - __main__ -     Num examples = 121489\n","01/26/2020 10:42:49 - INFO - __main__ -     Num Epochs = 3\n","01/26/2020 10:42:49 - INFO - __main__ -     Gradient Accumulation steps = 8\n","01/26/2020 10:42:49 - INFO - __main__ -     Total optimization steps = 1581\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Iteration:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-16d6c2ae0ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-37-dfb2a8550e6f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_dataloader, model, tokenizer)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         inputs = {\n\u001b[1;32m     85\u001b[0m             \u001b[0;34m\"encoder_input_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"]}]},{"cell_type":"code","metadata":{"id":"YNZZBZ7kYpIU","colab_type":"code","colab":{}},"source":["# batch.keys()\n","# dict_keys(['src', 'tgt', 'src_sent_labels', 'segs', 'clss', 'src_txt', 'tgt_txt'])\n","# data_loader.Dataloader(args, load_dataset(args, 'train', shuffle=True), args.batch_size, device,\n","#                                       shuffle=True, is_test=False)\n","\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size, drop_last=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"imiy6-1WYpLG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":440},"outputId":"2050a26c-ea9b-422c-c8ab-b6e1609501a3","executionInfo":{"status":"error","timestamp":1580284619749,"user_tz":-180,"elapsed":1666,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["# for _ in train_iterator:\n","#       epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n","\n","args.max_tgt_len = 96\n","train_dataloader = data_loader.Dataloader(args, load_dataset(args, 'train', shuffle=True), 512, args.device,\n","                                      shuffle=True, is_test=False)\n","\n","for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\", disable=False)):\n","  # print(step,batch)\n","\n","  # batch = tuple(t.to(args.device) for t in batch)\n","# setattr(self, 'src', src.to(device))\n","#             setattr(self, 'tgt', tgt.to(device))\n","#             setattr(self, 'segs', segs.to(device))\n","#             setattr(self, 'mask_src', mask_src.to(device))\n","#             setattr(self, 'mask_tgt', mask_tgt.to(device))\n","\n","  inputs = {\n","      \"encoder_input_ids\": batch.src,\n","      \"decoder_input_ids\": batch.tgt,\n","      \"token_type_ids\": batch.segs,\n","      \"encoder_attention_mask\": batch.mask_src,\n","      'decoder_attention_mask': batch.mask_tgt\n","  }\n","\n","  outputs = model(**inputs)\n","  # model outputs are always tuple in transformers (see doc)\n","  loss = outputs[0]\n","  print(loss)\n","\n","  break\n"," "],"execution_count":87,"outputs":[{"output_type":"stream","text":["01/29/2020 07:56:59 - INFO - __main__ -   Loading train dataset from data/arxiv.train.3.bert.pt, number of examples: 18185\n","Iteration: 0it [00:00, ?it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-87-80d03706e6db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   }\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0;31m# model outputs are always tuple in transformers (see doc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/trans_bertsum/modeling_bertabs.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask, decoder_attention_mask)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mencoder_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mdec_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_decoder_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/trans_bertsum/modeling_bertabs.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, encoder_hidden_states, state, attention_mask, memory_lengths, step, cache, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mprevious_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_layer_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0mlayer_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layer_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             )\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/trans_bertsum/modeling_bertabs.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, memory_bank, src_pad_mask, tgt_pad_mask, previous_input, layer_cache, step)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mquery_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         mid = self.context_attn(\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mmemory_bank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_bank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_pad_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         )\n\u001b[1;32m    343\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/trans_bertsum/modeling_bertabs.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, key, value, query, mask, layer_cache, type, predefined_graph_1)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;31m# 2) Calculate and scale scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/trans_bertsum/modeling_bertabs.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;34m\"\"\"  projection \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_per_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0munshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[271, -1, 8, 96]' is invalid for input of size 133632"]}]},{"cell_type":"code","metadata":{"id":"Ga-qb8jlYpOT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"91042765-fb70-4e8c-f0af-8167d5290c24","executionInfo":{"status":"ok","timestamp":1580284631208,"user_tz":-180,"elapsed":534,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["133632 / 8 / 96\n","# 112 * 8 * 96\n","# 78336 / (8 * 96)\n","# shape '[138, -1, 8, 96]' is invalid for input of size 18432\n","# [163, -1, 8, 96]"],"execution_count":88,"outputs":[{"output_type":"execute_result","data":{"text/plain":["174.0"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"id":"7Pq02QZoOM6U","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wa4I1VtYOM9W","colab_type":"code","colab":{}},"source":["Batch = namedtuple(\"Batch\", [\"document_names\", \"batch_size\", \"src\", \"segs\", \"mask_src\", \"tgt_str\"])\n","# encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask, decoder_attention_mask\n","\n","def build_data_iterator(tokenizer):\n","    dataset = load_and_cache_examples(tokenizer)\n","    sampler = SequentialSampler(dataset)\n","\n","    def collate_fn(data):\n","        return collate(data, tokenizer, block_size=512, device=args.device)\n","\n","    iterator = DataLoader(dataset, sampler=sampler, batch_size=args.train_batch_size, collate_fn=collate_fn,)\n","\n","    return iterator\n","\n","\n","def load_and_cache_examples(tokenizer):\n","    dataset = SummarizationDataset('/content/data/')\n","    return dataset\n","\n","\n","def collate(data, tokenizer, block_size, device):\n","    \"\"\" Collate formats the data passed to the data loader.\n","\n","    In particular we tokenize the data batch after batch to avoid keeping them\n","    all in memory. We output the data as a namedtuple to fit the original BertAbs's\n","    API.\n","    \"\"\"\n","    data = [x for x in data if not len(x[1]) == 0]  # remove empty_files\n","    names = [name for name, _, _ in data]\n","    summaries = [\" \".join(summary_list) for _, _, summary_list in data]\n","\n","    encoded_text = [encode_for_summarization(abstract, title, tokenizer) for _, abstract, title in data]\n","    encoded_stories = torch.tensor(\n","        [fit_to_block_size(abstract, block_size, tokenizer.pad_token_id) for abstract, _ in encoded_text]\n","    )\n","    encoder_token_type_ids = compute_token_type_ids(encoded_stories, tokenizer.cls_token_id)\n","    encoder_mask = build_mask(encoded_stories, tokenizer.pad_token_id)\n","\n","    batch = Batch(\n","        document_names=names,\n","        batch_size=len(encoded_stories),\n","        src=encoded_stories, #.to(device),\n","        segs=encoder_token_type_ids, #.to(device),\n","        mask_src=encoder_mask, #.to(device),\n","        tgt_str=summaries,\n","    )\n","\n","    return batch\n","\n","data_iterator = build_data_iterator(tokenizer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7kBLkrEnQeNM","colab_type":"code","colab":{}},"source":["for batch in data_iterator:\n","  print(batch[2][0])\n","  break"],"execution_count":0,"outputs":[]}]}