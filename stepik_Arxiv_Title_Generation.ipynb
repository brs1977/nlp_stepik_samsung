{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"stepik_Arxiv_Title_Generation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-EJ9rSJM822I","colab_type":"code","outputId":"bd0281c3-5a11-4c85-c101-48d3d9c7f8d7","executionInfo":{"status":"ok","timestamp":1578303013484,"user_tz":-180,"elapsed":32287,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive, files\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vWGylHHQ-drC","colab_type":"text"},"source":["#Kaggle env"]},{"cell_type":"code","metadata":{"id":"JwAa9DPs9rV0","colab_type":"code","outputId":"d792b2a5-c24f-4469-c56f-5a13eef7237f","executionInfo":{"status":"ok","timestamp":1578303020400,"user_tz":-180,"elapsed":39187,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#!pip install -q kaggle\n","\n","#kaggle key\n","!mkdir ~/.kaggle\n","!cp /content/drive/My\\ Drive/kaggle.json ~/.kaggle\n","!ls ~/.kaggle\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["kaggle.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SwHJhMYjNpzX","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fse0XhwIPYm2","colab_type":"text"},"source":["#Download data"]},{"cell_type":"code","metadata":{"id":"fRvCoKxt9rZE","colab_type":"code","outputId":"f0d70eb9-766f-4507-c1d9-872a06b469f2","executionInfo":{"status":"ok","timestamp":1578303035695,"user_tz":-180,"elapsed":11417,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["!kaggle competitions download -c title-generation"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n","Downloading vocs.pkl.zip to /content\n"," 78% 5.00M/6.39M [00:00<00:00, 22.7MB/s]\n","100% 6.39M/6.39M [00:00<00:00, 25.4MB/s]\n","Downloading test.csv to /content\n","  0% 0.00/905k [00:00<?, ?B/s]\n","100% 905k/905k [00:00<00:00, 59.5MB/s]\n","Downloading sample_submission.csv.zip to /content\n","  0% 0.00/778k [00:00<?, ?B/s]\n","100% 778k/778k [00:00<00:00, 106MB/s]\n","Downloading train.csv.zip to /content\n"," 94% 41.0M/43.7M [00:02<00:00, 12.1MB/s]\n","100% 43.7M/43.7M [00:02<00:00, 18.2MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"agocT_R9NsD4","colab_type":"code","outputId":"9ae37671-d20a-4e77-b57f-193f78554959","executionInfo":{"status":"ok","timestamp":1578303050933,"user_tz":-180,"elapsed":22978,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["!mkdir data\n","!unzip sample_submission.csv.zip -d data\n","!unzip vocs.pkl.zip -d data\n","!unzip train.csv.zip -d data\n","!mv test.csv data\n","\n","!ls data\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Archive:  sample_submission.csv.zip\n","  inflating: data/sample_submission.csv  \n","Archive:  vocs.pkl.zip\n","  inflating: data/vocs.pkl           \n","Archive:  train.csv.zip\n","  inflating: data/train.csv          \n","sample_submission.csv  test.csv  train.csv  vocs.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yLpjbZcnQFoM","colab_type":"code","colab":{}},"source":["!cat data/test.csv "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"djs1ZKj9PkIK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f8dbaeef-c06e-477b-a6ed-2d3d0442c6ab","executionInfo":{"status":"ok","timestamp":1578303054403,"user_tz":-180,"elapsed":3026,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["import pandas as pd\n","\n","train = pd.read_csv('data/train.csv', encoding='utf8')\n","test = pd.read_csv('data/test.csv', encoding='utf8')\n","\n","from sklearn.model_selection import train_test_split\n","\n","train, val = train_test_split(train, test_size=0.2)\n","\n","print(train.shape,val.shape,test.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(108000, 2) (27000, 2) (1000, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-AEgQOhXShT0","colab_type":"code","outputId":"c9a1fe62-4766-49dc-f137-6d2368861c46","executionInfo":{"status":"ok","timestamp":1578254350994,"user_tz":-180,"elapsed":900,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["train.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>title</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>we consider the problem of utility maximizatio...</td>\n","      <td>on optimal investment with processes of long o...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>in this paper we provide an explicit formula f...</td>\n","      <td>boolean complexes for ferrers graphs</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>kinesin-5, also known as eg5 in vertebrates is...</td>\n","      <td>relative velocity of sliding of microtubules b...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>we discuss the transition paths in a coupled b...</td>\n","      <td>bifurcation of transition paths induced by cou...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>two types of room temperature detectors of ter...</td>\n","      <td>all-electric detectors of the polarization sta...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            abstract                                              title\n","0  we consider the problem of utility maximizatio...  on optimal investment with processes of long o...\n","1  in this paper we provide an explicit formula f...               boolean complexes for ferrers graphs\n","2  kinesin-5, also known as eg5 in vertebrates is...  relative velocity of sliding of microtubules b...\n","3  we discuss the transition paths in a coupled b...  bifurcation of transition paths induced by cou...\n","4  two types of room temperature detectors of ter...  all-electric detectors of the polarization sta..."]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"fKCb8llhSlZr","colab_type":"code","outputId":"5d4fccea-56a1-4cf0-8564-3d41dc1a1518","executionInfo":{"status":"ok","timestamp":1578254366122,"user_tz":-180,"elapsed":1000,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["test.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Most sequence transformation models use recurr...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The doc2vec approach was introduced as an exte...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LSTM models can vary greatly depending on sequ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A joint learning process of alignment and tran...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Current unsupervised image-to-image translatio...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            abstract\n","0  Most sequence transformation models use recurr...\n","1  The doc2vec approach was introduced as an exte...\n","2  LSTM models can vary greatly depending on sequ...\n","3  A joint learning process of alignment and tran...\n","4  Current unsupervised image-to-image translatio..."]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"Y5fFlyCxZD5b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":218},"outputId":"67e19d2a-1d65-400b-a7d3-393ed4cdd80b","executionInfo":{"status":"ok","timestamp":1578304341077,"user_tz":-180,"elapsed":1060,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["train.title"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["133191            absence of magnetic ordering in niga_2s_4\n","106691    the almost sure behavior of certain spatial re...\n","17970     therapeutic hypothermia: quantification of the...\n","7132      bayesian dose-response analysis for epidemiolo...\n","24508     lifetime measurements of triaxial strongly def...\n","                                ...                        \n","125812    dimension reduction of clustering results in b...\n","50189     demystifying fixed k-nearest neighbor informat...\n","130735    mathematical analysis of long tail economy usi...\n","60979     characteristics of transposable element exoniz...\n","2876      multiplierless 16-point dct approximation for ...\n","Name: title, Length: 108000, dtype: object"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"1YpTPnd-njgn","colab_type":"text"},"source":["#BertSum prepare\n"]},{"cell_type":"code","metadata":{"id":"JbUtM3rgnmob","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VfCQpbmWnmjg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k_PPLrfIg8Mn","colab_type":"text"},"source":["#BERT test"]},{"cell_type":"code","metadata":{"id":"IEf5LD66otEw","colab_type":"code","colab":{}},"source":["class InputExample():\n","    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","    def __init__(self, guid, text_a, text_b=None):\n","        \"\"\"Constructs a InputExample.\n","        Args:\n","            guid: Unique id for the example.\n","            text_a: string. The untokenized text of the first sequence.\n","                For single sequence tasks, only this sequence must be specified.\n","            text_b: (Optional) string. The untokenized text of the second\n","                sequence. Only must be specified for sequence pair tasks.\n","            label: (Optional) string. The label of the example. This should be\n","                specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        self.src_txt = text_a\n","        self.tgt_txt = text_b\n","\n","def _create_examples(self, src_lines,tgt_lines,set_type):\n","    examples = [] \n","    for i,data in enumerate(zip(src_lines,tgt_lines)):\n","        guid = \"%s-%s\" % (set_type, i)\n","        if len(data[0])==0 or len(data[1])==0:\n","          continue\n","        src_lines = tokenization.convert_to_unicode(data[0][0])\n","        tgt_lines = tokenization.convert_to_unicode(data[1][0])\n","        examples.append(InputExample(guid=guid, text_a=src_lines,\n","                                  text_b=tgt_lines))\n","    return examples"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dv48u-CwotAf","colab_type":"code","colab":{}},"source":["# Load pre-trained model tokenizer (vocabulary)\n","# modelpath = \"bert-base-uncased\"\n","# tokenizer = BertTokenizer.from_pretrained(modelpath)\n","\n","set_type = 'train'\n","examples = [] \n","for row in train[:3].iterrows():\n","    i,(src_lines,tgt_lines) = row\n","    guid = \"%s-%s\" % (set_type, i)\n","    if len(src_lines)==0 or len(tgt_lines)==0:\n","      continue\n","    src_lines = tokenizer.tokenize(src_lines)\n","    tgt_lines = tokenizer.tokenize(tgt_lines)\n","    examples.append(InputExample(guid=guid, text_a=src_lines,\n","                              text_b=tgt_lines))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J2RDFOQq14IC","colab_type":"code","colab":{}},"source":["def _format_to_bert(params):\n","    json_file, args, save_file = params\n","    if (os.path.exists(save_file)):\n","        logger.info('Ignore %s' % save_file)\n","        return\n","\n","    bert = BertData(args)\n","\n","    logger.info('Processing %s' % json_file)\n","    jobs = json.load(open(json_file))\n","    datasets = []\n","    for d in jobs:\n","        source, tgt = d['src'], d['tgt']\n","        if (args.oracle_mode == 'greedy'):\n","            oracle_ids = greedy_selection(source, tgt, 3)\n","        elif (args.oracle_mode == 'combination'):\n","            oracle_ids = combination_selection(source, tgt, 3)\n","        b_data = bert.preprocess(source, tgt, oracle_ids)\n","        if (b_data is None):\n","            continue\n","        indexed_tokens, labels, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n","        b_data_dict = {\"src\": indexed_tokens, \"labels\": labels, \"segs\": segments_ids, 'clss': cls_ids,\n","                       'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n","        datasets.append(b_data_dict)\n","    logger.info('Saving to %s' % save_file)\n","    torch.save(datasets, save_file)\n","    datasets = []\n","    gc.collect()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m3j5QvZs14Wx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nsZuCTcI14Te","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"efXxvAPGwxPh","colab_type":"code","colab":{}},"source":["\n","#inpf = convert_single_example(1,examples[0],75,75,tokenizer)\n","inpf.src_segment_ids"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jJPSGqYStnpe","colab_type":"code","colab":{}},"source":["class InputFeatures():\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(self, src_input_ids,src_input_mask,src_segment_ids,tgt_input_ids,tgt_input_mask,tgt_labels):\n","        self.src_input_ids = src_input_ids\n","        self.src_input_mask = src_input_mask\n","        self.src_segment_ids = src_segment_ids\n","        self.tgt_input_ids = tgt_input_ids\n","        self.tgt_input_mask = tgt_input_mask \n","        self.tgt_labels = tgt_labels\n","        \n","def convert_single_example(ex_index, example, max_seq_length_src,max_seq_length_tgt,\n","                           tokenizer):\n","    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n","    \"\"\"\n","    label_map = {}\n","    for (i, label) in enumerate(label_list):\n","        label_map[label] = i\n","    \"\"\"\n","    # tokens_a = tokenizer.tokenize(example.src_txt)\n","    # tokens_b = tokenizer.tokenize(example.tgt_txt)\n","\n","    tokens_a = example.src_txt\n","    tokens_b = example.tgt_txt\n","\n","\n","    # Modifies `tokens_a` and `tokens_b` in place so that the total\n","    # length is less than the specified length.\n","    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","    if len(tokens_a) > max_seq_length_src - 2:\n","            tokens_a = tokens_a[0:(max_seq_length_src - 2)]\n","    \n","    if len(tokens_b) > max_seq_length_tgt - 2:\n","            tokens_b = tokens_b[0:(max_seq_length_tgt - 2)]\n","\n","    \n","    tokens_src = []\n","    segment_ids_src = []\n","    tokens_src.append(\"[CLS]\")\n","    segment_ids_src.append(0)\n","    for token in tokens_a:\n","        tokens_src.append(token)\n","        segment_ids_src.append(0)\n","    tokens_src.append(\"[SEP]\")\n","    segment_ids_src.append(0)\n","  \n","\n","    tokens_tgt = []\n","    segment_ids_tgt = []\n","    tokens_tgt.append(\"[CLS]\")\n","    #segment_ids_tgt.append(0)\n","    for token in tokens_b:\n","        tokens_tgt.append(token)\n","        #segment_ids_tgt.append(0)\n","    tokens_tgt.append(\"[SEP]\")\n","    #segment_ids_tgt.append(0)\n","\n","    input_ids_src = tokenizer.convert_tokens_to_ids(tokens_src)\n","   \n","    \n","\n","    input_ids_tgt = tokenizer.convert_tokens_to_ids(tokens_tgt)\n","    \n","    #Adding begiining and end token\n","    input_ids_tgt = input_ids_tgt[:-1] \n","    \n","    input_mask_src = [1] * len(input_ids_src)\n","\n","\n","    input_mask_tgt = [1] * len(input_ids_tgt)\n","    \n","    labels_tgt = input_ids_tgt[1:]\n","    \n","    \n","    labels_tgt.append(0)\n","    \n","    #print(len(input_ids_tgt))\n","    #print(len(input_mask_tgt))\n","    #print(len(labels_tgt))\n","    #print(len(segment_ids_tgt))\n","    \n","    while len(input_ids_src) < max_seq_length_src:\n","        input_ids_src.append(0)\n","        input_mask_src.append(0)\n","        segment_ids_src.append(0)\n","\n","    while len(input_ids_tgt) < max_seq_length_tgt:\n","        input_ids_tgt.append(0)\n","        input_mask_tgt.append(0)\n","        segment_ids_tgt.append(0)\n","        labels_tgt.append(0)\n","\n","    feature = InputFeatures( src_input_ids=input_ids_src,src_input_mask=input_mask_src,src_segment_ids=segment_ids_src,\n","        tgt_input_ids=input_ids_tgt,tgt_input_mask=input_mask_tgt,tgt_labels=labels_tgt)\n","\n","    \n","    return feature"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pr69yB2wg-Jn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":252},"outputId":"b951f3d6-b93c-4bb6-8859-3dcf0dfad06e","executionInfo":{"status":"ok","timestamp":1578308935100,"user_tz":-180,"elapsed":6168,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["import torch\n","from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM\n","\n","# Load pre-trained model tokenizer (vocabulary)\n","modelpath = \"bert-base-uncased\"\n","tokenizer = BertTokenizer.from_pretrained(modelpath)\n","\n","text = \"dummy. although he had already eaten a large meal, he was still very hungry.\"\n","target = \"hungry\"\n","tokenized_text = tokenizer.tokenize(text)\n","\n","# Mask a token that we will try to predict back with `BertForMaskedLM`\n","masked_index = tokenized_text.index(target)\n","tokenized_text[masked_index] = '[MASK]'\n","\n","# Convert token to vocabulary indices\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n","segments_ids = [1] * len(tokenized_text)\n","# this is for the dummy first sentence. \n","segments_ids[0] = 0\n","segments_ids[1] = 0\n","\n","# Convert inputs to PyTorch tensors\n","tokens_tensor = torch.tensor([indexed_tokens])\n","segments_tensors = torch.tensor([segments_ids])\n","# Load pre-trained model (weights)\n","model = BertForMaskedLM.from_pretrained(modelpath)\n","model.eval()\n","\n","# Predict all tokens\n","predictions = model(tokens_tensor, segments_tensors)\n","predicted_index = torch.argmax(predictions[0][0, masked_index]).item()\n","predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n","\n","print(\"Original:\", text)\n","print(\"Masked:\", \" \".join(tokenized_text))\n","\n","print(\"Predicted token:\", predicted_token)\n","print(\"Other options:\")\n","# just curious about what the next few options look like.\n","for i in range(10):\n","    predictions[0][0,masked_index,predicted_index] = -11100000\n","    predicted_index = torch.argmax(predictions[0][0, masked_index]).item()\n","    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n","    print(predicted_token)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Original: dummy. although he had already eaten a large meal, he was still very hungry.\n","Masked: dummy . although he had already eaten a large meal , he was still very [MASK] .\n","Predicted token: ['hungry']\n","Other options:\n","['strong']\n","['tired']\n","['weak']\n","['angry']\n","['concerned']\n","['alert']\n","['nervous']\n","['pale']\n","['.']\n","['excited']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6kNvX1GEg-RH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"d81d2f30-ea00-4db9-8549-072d22e44862","executionInfo":{"status":"ok","timestamp":1578308835409,"user_tz":-180,"elapsed":1075,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["#predicted_index = torch.argmax(predictions[0, masked_index]).item()\n","predictions[0][0,15] #, masked_index]"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-5.0937, -5.0305, -5.1461,  ..., -3.5266, -4.0072, -3.8648],\n","       grad_fn=<SelectBackward>)"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"0LqVXwzWg-Nl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S-KVZk66c_Yt","colab_type":"text"},"source":["#Summarizer"]},{"cell_type":"code","metadata":{"id":"SUKoRKmudM53","colab_type":"code","colab":{}},"source":["!pip install bert-extractive-summarizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVoB046RdC1H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"bb71827f-82b6-4e77-d495-410710a33e19","executionInfo":{"status":"ok","timestamp":1578307684069,"user_tz":-180,"elapsed":115339,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["from summarizer import Summarizer\n","\n","model = Summarizer()\n","\n","model(train.abstract[0])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["100%|██████████| 40155833/40155833 [00:02<00:00, 18096237.37B/s]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["'we consider the problem of utility maximization for investors with power utility functions. in such models there is no hope to get a formula for the achievable maximal utility.'"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"HFDjoSgYeGJ3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d20f798a-b9a6-45b6-ac6c-972aa66ee4b7","executionInfo":{"status":"ok","timestamp":1578307771172,"user_tz":-180,"elapsed":1051,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["train.title[0]"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'on optimal investment with processes of long or negative memory'"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"njmrtdR-dC8G","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mL6aA9E9A5HT","colab_type":"text"},"source":["#BERT"]},{"cell_type":"code","metadata":{"id":"WzXhzeaR-z_s","colab_type":"code","colab":{}},"source":["!pip install pytorch-transformers"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j6RrxPBjAuzC","colab_type":"code","outputId":"e4425363-22be-4148-94b2-7cb176db4439","executionInfo":{"status":"ok","timestamp":1578303081098,"user_tz":-180,"elapsed":5734,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}},"colab":{"base_uri":"https://localhost:8080/","height":79}},"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from pytorch_transformers import BertTokenizer, BertConfig\n","from pytorch_transformers import AdamW, BertForSequenceClassification\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import io\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ObpT_KnwBuTt","colab_type":"code","outputId":"827944b6-e0f4-4aa2-85a3-0664b09a6368","executionInfo":{"status":"ok","timestamp":1578303140945,"user_tz":-180,"elapsed":1107,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if device == 'cpu':\n","    print('cpu')\n","else:\n","    n_gpu = torch.cuda.device_count()\n","    print(torch.cuda.get_device_name(0))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GYEwDxsXBuZU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f4f32853-ef30-419b-d0aa-dd42d483e10b","executionInfo":{"status":"ok","timestamp":1578303279431,"user_tz":-180,"elapsed":1994,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}}},"source":["# vocab_file = 'data/vocs.pkl'\n","# tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=True) \n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["100%|██████████| 231508/231508 [00:00<00:00, 901877.04B/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"6S6a9CgLBucr","colab_type":"code","colab":{}},"source":["tokenized_texts = [tokenizer.tokenize(sent) for sent in train.abstract.values]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l9F0s5oIBuW6","colab_type":"code","outputId":"c20281bc-afc2-494c-986f-0c9e44dd2b0c","executionInfo":{"status":"ok","timestamp":1578306613336,"user_tz":-180,"elapsed":1716,"user":{"displayName":"Bukin Ruslan","photoUrl":"","userId":"02589038028886526730"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(tokenized_texts[0])"],"execution_count":16,"outputs":[{"output_type":"stream","text":["['triangular', '-', 'layered', 'ni', '##ga', '##2', '##s', '##4', ',', 'contrary', 'to', 'intuitive', 'expectation', ',', 'does', 'not', 'form', 'a', 'non', '##coll', '##ine', '##ar', 'anti', '##fer', '##rom', '##agne', '##tic', 'structure', ',', 'as', 'do', 'iso', '##ele', '##ct', '##ron', '##ic', 'na', '##cr', '##o', '##2', 'and', 'li', '##cr', '##o', '##2', '.', 'instead', ',', 'the', 'local', 'magnetic', 'moments', 'remain', 'disorder', '##ed', 'down', 'to', 'the', 'lowest', 'measured', 'temperature', '.', 'to', 'get', 'more', 'insight', 'into', 'this', 'phenomenon', ',', 'we', 'have', 'performed', 'first', 'principles', 'calculations', 'of', 'the', 'first', ',', 'second', 'end', 'third', 'neighbors', 'exchange', 'interactions', ',', 'and', 'found', 'that', 'the', 'second', 'neighbor', 'exchange', 'is', 'ne', '##gli', '##gible', ',', 'while', 'the', 'first', 'and', 'the', 'third', 'neighbor', 'exchanges', 'are', 'comparable', 'and', 'anti', '##fer', '##rom', '##agne', '##tic', '.', 'both', 'are', 'rapidly', 'suppressed', 'by', 'the', 'on', '-', 'site', 'hubbard', 'rep', '##ulsion', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p84GRBZFbpqe","colab_type":"code","colab":{}},"source":["MAX_LEN = 1500\n","\n","input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n","                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xW2hqI6CR0Ad","colab_type":"code","colab":{}},"source":["attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lv3oQIZWaAa7","colab_type":"code","colab":{}},"source":["attention_masks[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2V3yUvGuBj9i","colab_type":"text"},"source":["#Seq2Seq"]},{"cell_type":"code","metadata":{"id":"-6smidgAAuxD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"16vurCXP-0I1","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchtext.data import Field, BucketIterator\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","import spacy\n","\n","import random\n","import math\n","import time\n","\n","SEED = 1234\n","\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","spacy_en = spacy.load('en')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rG0iqkf-0Mb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T4PtORca-0PG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}